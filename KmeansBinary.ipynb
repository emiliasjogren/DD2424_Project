{"cells":[{"cell_type":"markdown","metadata":{"id":"52872W-vO5Hk"},"source":["# Paws and Prediction\n","## Overview of the project\n","0. Import libraries\n","1. Load and transform data\n","2. Visualize data\n","3. Import model\n","4. Perform feature analysis of model output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L1E40tpPOpHD"},"outputs":[],"source":["import torch\n","from torchvision.datasets import OxfordIIITPet\n","from torchvision import transforms, datasets, models\n","from torch.utils.data import DataLoader, random_split\n","from torchvision.transforms.functional import InterpolationMode\n","from torchvision.utils import make_grid\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from torch import nn, optim\n","from torch.utils.data import Subset\n","from sklearn.decomposition import PCA\n","from sklearn.cluster import KMeans\n","\n","import time"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1716194284119,"user":{"displayName":"Agaton Domberg","userId":"02908242250402386696"},"user_tz":-120},"id":"XG_ZaxALO_to","outputId":"73d6463a-45b5-4a12-d651-0e7172702c9b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using gpu: True \n"]}],"source":["## for gpu usage, check true\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","print('Using gpu: %s ' % torch.cuda.is_available())"]},{"cell_type":"markdown","metadata":{"id":"_sUyleu2PBll"},"source":["## 1. Load and transform data\n","\n","We might want to think about how we transform the data and the splitting\n","Also about the size (now [3, 384, 384]), will probably require a lot of compute to train? Look up what seems reasonable\n","\n","\"\"The inference transforms are available at EfficientNet_B4_Weights.IMAGENET1K_V1.transforms and perform the following preprocessing operations: Accepts PIL.Image, batched (B, C, H, W) and single (C, H, W) image torch.Tensor objects. The images are resized to resize_size=[384] using interpolation=InterpolationMode.BICUBIC, followed by a central crop of crop_size=[380]. Finally the values are first rescaled to [0.0, 1.0] and then normalized using mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225].\"\"\n","\n","\n","####ser ut som att man kan addera random augmentations :))\n","\n","def get_train_transform(IMAGE_SIZE, pretrained):\n","    train_transform = transforms.Compose([\n","        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n","        transforms.RandomHorizontalFlip(p=0.5),\n","        transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n","        transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.5),\n","        transforms.ToTensor(),\n","        normalize_transform(pretrained)\n","    ])\n","    retur"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GpAVPc7DPAKY"},"outputs":[],"source":["#TODO - load data much faster\n","\n","class CustomOxfordPets(OxfordIIITPet):\n","    def __init__(self, *args, label_map=None, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        self.label_map = label_map\n","\n","    def __getitem__(self, index):\n","        image, target = super().__getitem__(index)\n","        if int(target) in self.label_map:\n","            target = self.label_map[target]\n","        return image, target\n","class LoadData:\n","    def __init__(self, binary=True):\n","        self.transform = self.transform_images()\n","\n","        if binary:\n","            self.classes = 2  # Might not be necessary\n","            self.label_map = self.initialize_label_map()  # Initialize label_map for binary classification\n","            self.bin_dataset()\n","        else:\n","            self.classes = 37\n","            self.mult_dataset()\n","\n","    def initialize_label_map(self):\n","        \"\"\"Initialize the label map for binary classification.\"\"\"\n","        # Dictionary mapping original labels to binary labels\n","        label_dict = {\n","            0: 0, 1: 1, 2: 1, 3: 1, 4: 1, 5: 0,\n","            6: 0, 7: 0, 8: 1, 9: 0, 10: 1, 11: 0, 12: 1,\n","            13: 1, 14: 1, 15: 1, 16: 1, 17: 1, 18: 1,\n","            19: 1, 20: 0, 21: 1, 22: 1, 23: 0, 24: 1,\n","            25: 1, 26: 0, 27: 0, 28: 1, 29: 1, 30: 1,\n","            31: 1, 32: 0, 33: 0, 34: 1, 35: 1, 36: 1\n","        }\n","        return label_dict\n","\n","    def bin_dataset(self):\n","        \"\"\"Load the binary dataset.\"\"\"\n","        # Use the initialized label map\n","        self.dataset_trainval = CustomOxfordPets(\n","            root='./data', transform=self.transform, download=True, label_map=self.label_map, split='trainval'\n","        )\n","        self.test_dataset = CustomOxfordPets(\n","            root='./data', transform=self.transform, download=True, label_map=self.label_map, split='test'\n","        )\n","\n","    def mult_dataset(self):\n","        \"\"\"Load the multiclass dataset.\"\"\"\n","        self.dataset_trainval = OxfordIIITPet(\n","            root='./data', transform=self.transform, download=True, split='trainval'\n","        )\n","        self.test_dataset = OxfordIIITPet(\n","            root='./data', transform=self.transform, download=True, split='test'\n","        )\n","\n","    def data_loaders(self, batch_size, split_ratio, labeled_percent = 1):\n","        \"\"\"Initialize dataloaders.\"\"\"\n","        train_size = int(split_ratio * len(self.dataset_trainval))\n","\n","        val_size = len(self.dataset_trainval) - train_size\n","\n","        train_dataset, val_dataset = random_split(self.dataset_trainval, [train_size, val_size])\n","\n","        self.train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","        if labeled_percent <1:\n","          print(\"true size = \" + str(train_size))\n","          pseudo_train_size = int(labeled_percent *train_size)\n","\n","          print(\"labeled train size = \" + str(pseudo_train_size))\n","\n","\n","          pseudo_guess_size =  train_size -pseudo_train_size\n","\n","          print(\"unlabeled size = \" + str(pseudo_guess_size))\n","\n","          train_pseudo, guess_pseudo = random_split(train_dataset, [pseudo_train_size, pseudo_guess_size])\n","\n","\n","\n","          self.trainPseudo_loader =  DataLoader(train_pseudo, batch_size=batch_size, shuffle=True)\n","\n","\n","          self.guess_loader = DataLoader(guess_pseudo, batch_size=batch_size, shuffle=True)\n","\n","\n","        else:\n","          self.trainPseudo_loader = [];\n","          self.guess_loader = [];\n","\n","\n","\n","        self.val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n","        self.test_loader = DataLoader(self.test_dataset, batch_size=batch_size, shuffle=False)\n","\n","    def transform_images(self):\n","        transform = transforms.Compose([\n","            transforms.Resize(384, interpolation=InterpolationMode.BICUBIC),\n","            transforms.CenterCrop(380),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","        ])\n","        return transform\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hdQ_sVv6b-mH"},"source":["## 2. Train Class to initialize all training"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":47711,"status":"ok","timestamp":1716194331826,"user":{"displayName":"Agaton Domberg","userId":"02908242250402386696"},"user_tz":-120},"id":"5dGfAKnaP0wO","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a43a39c0-b95c-4c08-a9b8-3b1e4e388a67"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://thor.robots.ox.ac.uk/datasets/pets/images.tar.gz to data/oxford-iiit-pet/images.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 791918971/791918971 [00:27<00:00, 29220584.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/oxford-iiit-pet/images.tar.gz to data/oxford-iiit-pet\n","Downloading https://thor.robots.ox.ac.uk/datasets/pets/annotations.tar.gz to data/oxford-iiit-pet/annotations.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 19173078/19173078 [00:01<00:00, 13980404.23it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/oxford-iiit-pet/annotations.tar.gz to data/oxford-iiit-pet\n"]}],"source":["\n","class Train:\n","    def __init__(self, lr: float, bs: int, epochs: int, split_ratio: float, binary: bool, model, _LoadData=LoadData(), labeled_percent = 1):\n","        self.lr = lr\n","        self.batch_size = bs\n","        self.split_ratio = split_ratio\n","        self.epochs = epochs\n","        self.binary = binary\n","\n","        self.labeled_percent = labeled_percent\n","        self.data = _LoadData #kom på snyggare variablenamn\n","        self.data.data_loaders(self.batch_size, self.split_ratio, labeled_percent=self.labeled_percent)\n","\n","        self.train_loader = self.data.train_loader\n","        self.val_loader = self.data.val_loader\n","        self.test_loader = self.data.test_loader\n","\n","        self.train_pseudo_loader = self.data.trainPseudo_loader\n","\n","        self.guess_loader = self.data.guess_loader\n","\n","        self.model = self.load_model()\n","        if model is not None:\n","            self.model = torch.load(model) #kolla upp hur vi laddar in färdigtränade modeller för detta funkar inte\n","\n","        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") #torch.device(\"cpu\") #\n","        print('Using gpu: %s ' % torch.cuda.is_available())\n","\n","        self.validation_losses = [] #eller vill jag initialisera det i träningsfunktionen?\n","        self.training_losses = []\n","\n","        self.loss_func = nn.BCEWithLogitsLoss() #fixa för binare\n","        self.optimizer = optim.Adam(self.model.classifier.parameters(), lr=self.lr)\n","\n","\n","    def load_model(self):\n","        \"\"\"Load the weights of the model\"\"\"\n","        model_b4 = models.efficientnet_b4(weights = models.EfficientNet_B4_Weights.IMAGENET1K_V1)\n","\n","        # TODO - fundera på om vi vill frysa alla lager\n","        for param in model_b4.parameters(): #Freeze\n","            param.requires_grad = False\n","\n","        num_features = model_b4.classifier[1].in_features\n","\n","        # Replace the classifier layer\n","        model_b4.classifier[1] = nn.Linear(num_features, 1)\n","\n","        return model_b4\n","\n","    def pseudo_labels(self):\n","        temp_model = self.load_model()\n","        temp_model.classifier = torch.nn.Identity()\n","        temp_model.eval()\n","        temp_model.to(self.device)\n","        modified_dataset = []\n","\n","        start_time = time.time()  # Start timing\n","\n","        modified_dataset = []\n","\n","        list_of_outputs = []\n","        labels = []\n","        count = 0\n","        with torch.no_grad():\n","            for inputs, _ in self.train_loader: #OBS: Vi tittar ej på targets\n","                inputs = inputs.to(device)\n","                outputs = temp_model(inputs)\n","                list_of_outputs.append(outputs.cpu())\n","\n","        list_of_outputs = torch.cat(list_of_outputs, dim=0)\n","        kmeans = KMeans(n_clusters=2, random_state=0)\n","        cluster_labels = kmeans.fit_predict(list_of_outputs)\n","\n","        temp_sum = 0\n","        compare = 0\n","        with torch.no_grad():\n","            for inputs, labels in self.train_pseudo_loader:\n","                inputs = inputs.to(device)\n","                outputs = temp_model(inputs).cpu()\n","                #two_D_outputs = pca.transform(outputs)\n","                pred_labels = kmeans.predict(outputs)\n","                temp_sum += sum(np.array(pred_labels) == np.array(labels))\n","                #compare += len(labels)\n","        #print(temp_sum)\n","        #print(compare)\n","        if temp_sum < compare/2:\n","          label_dict = {0: 1, 1:0}\n","        else:\n","          label_dict = {0:0, 1:1}\n","\n","\n","        count_similar = 0\n","\n","        with torch.no_grad():\n","            for images, _ in self.train_loader: #OBS: Vi tittar ej på targets\n","                inputs = images.to(device)\n","                outputs_model = temp_model(inputs).cpu()\n","                #two_D_outputs = pca.transform(outputs_model)\n","                pred_labels = kmeans.predict(outputs_model)\n","                outputs = pred_labels\n","                images = images.to(\"cpu\")\n","                j = 0\n","                for image in images:\n","\n","                  modified_dataset.append((image, label_dict[outputs[j]]))\n","                  j += 1\n","\n","\n","        #shuffle true eller false här?\n","        self.train_loader = DataLoader(modified_dataset, batch_size=self.batch_size, shuffle=True)\n","\n","\n","    def train(self):\n","        \"\"\"Train the network\"\"\"\n","\n","        self.model.train() #Bör inte störa \"freezed\" layers\n","        self.model.to(self.device)\n","\n","        start_time = time.time()  # Start timing\n","\n","        for epoch in range(self.epochs):\n","            epoch_loss = 0.0  # Initialize epoch loss\n","            for images, labels in self.train_loader:\n","                labels = labels.float()  #Läste att det kunde vara viktigt\n","\n","                images, labels = images.to(self.device), labels.to(self.device)\n","                #print(labels)\n","                self.optimizer.zero_grad()\n","                outputs = self.model(images)\n","                loss = self.loss_func(outputs, labels.unsqueeze(1))\n","                loss.backward()\n","                self.optimizer.step()\n","\n","                epoch_loss += loss.item()\n","\n","            # Compute validation loss\n","            avg_epoch_loss = epoch_loss / len(self.train_loader)\n","            val_loss = self.compute_validation_loss()\n","            print(f'Epoch {epoch+1}, Training Loss: {avg_epoch_loss} \\t Validation Loss: {val_loss}')\n","\n","            self.training_losses.append(avg_epoch_loss)\n","            self.validation_losses.append(val_loss)\n","\n","        end_time = time.time()  # End timing\n","        elapsed_time = end_time - start_time\n","        print(f'Training finished in {elapsed_time:.2f} seconds')\n","\n","        accuracy = self.evaluate()\n","\n","        save_path = f'model_bs{self.batch_size}_lr{self.lr}_acc{np.round(accuracy, 4)}.pt'\n","        torch.save(self.model.state_dict(), save_path) # är det såhär vi vill spara modeller? Eller bara spara vikter i vårt lager?\n","        print(f\"Model saved at '{save_path}'\")\n","\n","        return accuracy\n","\n","\n","    def compute_validation_loss(self):\n","        \"\"\"Compute the loss on the validation set\"\"\"\n","        self.model.eval()\n","        val_loss = 0.0\n","        with torch.no_grad():\n","            for images, labels in self.val_loader:\n","                labels = labels.float()\n","                images, labels = images.to(self.device), labels.to(self.device)\n","                outputs = self.model(images)\n","                loss = self.loss_func(outputs, labels.unsqueeze(1))\n","                val_loss += loss.item() * images.size(0)  # Multiply by batch size to account for varying batch sizes\n","        val_loss /= len(self.val_loader.dataset)  # Divide by total number of validation samples\n","        self.model.train()\n","        return val_loss\n","\n","    #funkar endast på binary dataset, skapa en till funktion? eller nåt if statement\n","    def evaluate(self): #TODO - går så brutalt långsamt, varför\n","        \"\"\"Compute the accuracy of the model\"\"\"\n","        self.model.eval()\n","        count_similar = 0\n","        for images, labels in self.test_loader:\n","              #print(labels)\n","              labels = labels.float()  #Läste att det kunde vara viktigt\n","\n","              images, labels = images.to(self.device), labels.to(self.device)\n","              outputs = [1 if output > 0.5 else 0 for output in torch.sigmoid(self.model(images))]\n","              labels_cpu = labels.cpu()  # Move to CPU\n","              labels_np = labels_cpu.numpy()\n","              count_similar += np.sum(np.array([1 if label > 0.5 else 0 for label in labels_np]) == np.array(outputs))\n","\n","        accuracy = count_similar/(len(self.test_loader)*self.batch_size)\n","\n","        print(\"Accuracy on test set: \", np.round(accuracy, 4))\n","\n","        return np.round(accuracy, 4)\n","\n"]},{"cell_type":"markdown","source":["# K-means run"],"metadata":{"id":"VuXOvoksXeeY"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":994638,"status":"ok","timestamp":1716196139791,"user":{"displayName":"Agaton Domberg","userId":"02908242250402386696"},"user_tz":-120},"id":"YKUqTVrDtJOJ","outputId":"0700d63f-a3a8-47f8-ad59-900d1e435c9f"},"outputs":[{"output_type":"stream","name":"stdout","text":["true size = 2944\n","labeled train size = 29\n","unlabeled size = 2915\n","Using gpu: True \n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["29\n","29\n","Epoch 1, Training Loss: 0.42390454981638037 \t Validation Loss: 0.30980203867606493\n","Epoch 2, Training Loss: 0.22318633736880578 \t Validation Loss: 0.20029697194695473\n","Epoch 3, Training Loss: 0.16273100171035723 \t Validation Loss: 0.13997929000660128\n","Epoch 4, Training Loss: 0.13644226754853583 \t Validation Loss: 0.11775481077316015\n","Epoch 5, Training Loss: 0.12114921688486863 \t Validation Loss: 0.09214107356155696\n","Epoch 6, Training Loss: 0.10349188435255834 \t Validation Loss: 0.08092344240487917\n","Epoch 7, Training Loss: 0.09826500561726077 \t Validation Loss: 0.0679983485530576\n","Epoch 8, Training Loss: 0.09247445941993805 \t Validation Loss: 0.06645629010365708\n","Epoch 9, Training Loss: 0.08447076550953131 \t Validation Loss: 0.05392550795501017\n","Epoch 10, Training Loss: 0.07828910330072572 \t Validation Loss: 0.04760298908323697\n","Epoch 11, Training Loss: 0.08083131039798584 \t Validation Loss: 0.04917869884925692\n","Epoch 12, Training Loss: 0.07719514288043644 \t Validation Loss: 0.03856950100389835\n","Epoch 13, Training Loss: 0.07078414878544761 \t Validation Loss: 0.04111756858132456\n","Epoch 14, Training Loss: 0.07274350309782439 \t Validation Loss: 0.03806453262982161\n","Epoch 15, Training Loss: 0.06938331709407856 \t Validation Loss: 0.03400449220941443\n","Epoch 16, Training Loss: 0.06941214272418343 \t Validation Loss: 0.0369284927956355\n","Training finished in 784.46 seconds\n","Accuracy on test set:  0.9869\n","Model saved at 'model_bs8_lr0.0005_acc0.9869.pt'\n"]},{"output_type":"execute_result","data":{"text/plain":["0.9869"]},"metadata":{},"execution_count":6}],"source":["bs = 8\n","lr = 0.0005\n","labeled_percent = 0.01\n","_LoadData = LoadData(binary=True)\n","train = Train(lr=lr, bs=bs, epochs=16, split_ratio=0.8, binary=True, model=None, _LoadData=_LoadData, labeled_percent=labeled_percent)\n","train.pseudo_labels()\n","train.train()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"collapsed_sections":["L2yq831zd7ja"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}